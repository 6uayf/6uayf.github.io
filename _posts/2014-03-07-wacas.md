---
layout: post
title: Notes from WACAS
excerpt: |
    I helped run [WACAS 2014][], a workshop on approximate computing, this
    week. Here are a few notes on broad topics that came up at the workshop.

    [wacas 2014]: http://sampa.cs.washington.edu/wacas14/
---
The world's first workshop---that I know of!---on approximate computing was this
week. [WACAS 2014][] was a great success. We had a great program of 19 papers and dozens of spectators in attendance.

[wacas 2014]: http://sampa.cs.washington.edu/wacas14/

Here are a few notes on some topics that came up during the workshop. (This isn't meant to be my "best papers" list; just a list of those moments I had thoughts worth writing about.)

## Infrastructure

Approximate computing as a research topic is picking up steam. As it gains
popularity, the need for reusable infrastructure components---compilers,
simulators, benchmarks, and the like---will grow.

There were two projects that reused the aging [EnerJ infrastructure][enerj] for experimental evaluation: [ExpAX][] from Georgia Tech and [load value approximation][lva] from Toronto. My heart goes out to those poor souls who had to cope with the duct-tape-and-baling-twine monstrosity that I produced as a first- and second-year grad student.

[expax]: http://sampa.cs.washington.edu/wacas14/papers/park.pdf
[lva]: http://sampa.cs.washington.edu/wacas14/papers/sanmiguel.pdf
[enerj]: http://sampa.cs.washington.edu/research/approximation/enerj.html

There's also a new infrastructure on the block: [iACT][] from Intel is an LLVM-based system for simulating some approximation strategies.

## Customers

[Lakshminarayana's paper][ibm] was partially about finding "customers" for approximation within IBM and the predictably frustrating challenges involved in convincing people to take it on. Asit, in the context of the [iACT paper][iact], mentioned disheartening survey results suggesting that programmers are conservative when confronted with the idea of approximate programming.

[iact]: http://sampa.cs.washington.edu/wacas14/papers/mishra.pdf
[ibm]: http://sampa.cs.washington.edu/wacas14/papers/renganarayana.pdf

## Definitions

The discussion (a "panel" with no panelists) at the end of the day was varied, entropic, and  fascinating. Predictably, the topic turned at one point to definitions: what is approximate computing, anyway?

Definitional disagreements are not worth spending too much time on, but I thought I'd try my hand at a succinct description in light of that discussion:

Approximate computing is about *abstractions that include error*. In
approximate computing, systems hope to improve efficiency by propagating error
to other layers. (This includes hardware exposing errors to software or
software to other software.) Those other layers then need to deal with
error---either by suppressing it or passing it down the line, possibly to the end user.

If your interface promises that errors effectively never happen---if it
encapsulates any potential misbehavior---then you're not doing approximate computing.

## Connection to Resilience

An important challenge in approximate computing is understanding quality degradations at run time. It would be useful for many reasons to raise an exception when things fall outside of  a prescribed error bound. [Beayna's paper][ucla] brought this issue up. [Sarita][] suggested that we take inspiration from hardware resilience technique like [symptom-based detection][].

[ucla]: http://sampa.cs.washington.edu/wacas14/papers/grigorian.pdf
[sarita]: http://rsim.cs.illinois.edu/~sadve/
[symptom-based detection]: http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=1467777

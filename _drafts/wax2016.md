---
title: "Notes on WAX 2016"
excerpt: |
    [WAX][] is the workshop on approximate computing. This year at [ASPLOS][], I organized the third or fourth iteration, depending on how you count, along with [Luis Ceze][luis], [Hadi Esmaeilzadeh][hadi], and [Ben Zorn][ben]. Here's some stuff that happened at the workshop.

    [luis]: http://homes.cs.washington.edu/~luisceze/
    [hadi]: http://www.cc.gatech.edu/~hadi/
    [ben]: http://research.microsoft.com/en-us/people/zorn/
    [wax]: http://approximate.computer/wax2016/
    [asplos]: https://www.ece.cmu.edu/calcm/asplos2016/
---
We held [WAX][], the workshop on approximate computing, at [ASPLOS][] last week. I love organizing WAX---it's been a great excuse for the approximation community to talk about the broader themes that extend beyond any single person's research project du jour.

Here are some notes on those themes.
You can also check out [the archived program][program] for links to papers and slides.

[wax]: http://approximate.computer/wax2016/
[asplos]: https://www.ece.cmu.edu/calcm/asplos2016/
[program]: http://approximate.computer/wax2016/program/

### Disgruntled Introductions

To introduce themselves, we had everyone say something they liked about approximate computing and something they didn't like.
Predictably, this invited a healthy dose of griping.
Two gripey themes emerged:

- People are unhappy that approximation hasn't "hit it big" yet, commercially speaking. We're a half decade or so into the approximate-computing craze already, so it feels to many like we should see shipping hardware sometime soon.
- Many people complained about the confusing terminology. What does *quality* mean versus *accuracy* versus *quality of service?* A cohort even complained about *approximate computing* itself being more popular than alternatives like *inexact* or *good-enough* computing.

<img class="img-responsive" src="{{site.url}}/media/wax2016.jpg" alt="looks like fun!">

### Cross-Stack Keynotes

We had two awesome keynote speakers, both of whom had interdisciplinary views on approximation.

[Matthai Philipose][matthai] from MSR has a goal of *continuous mobile vision:* always-on CV on a wearable device with all-day battery life and reasonable cloud costs.
He has data suggesting that approximation is critical---not just a luxury---for this setting: current vision techniques can't fit in the energy and dollar budgets you'd need. It's not even close.
So his project introduces approximation everywhere, from the camera sensor hardware to the DNN models and algorithms.

[Naveen Verma][naveen] from Princeton is a hardware researcher but, unlike some architects, believes approximate computing should come top down, from algorithms.
He showed off *data-driven hardware resilience*, where you train a machine learning model to counteract the effects of deterministic hardware approximation.
This cross-stack approach can lead to extremely good tolerance under the right conditions.

[matthai]: http://research.microsoft.com/en-us/people/matthaip/
[naveen]: http://ee.princeton.edu/people/faculty/naveen-verma

### Best Practices

The discussion at the end of the day coalesced around standards of rigor---or lack thereof---in approximate-computing research.
There was a broad consensus in particular that evaluation methodologies have not improved enough since those heady days of the first few approximation papers.
We agreed that there's a strong need for a *best practices* document for approximation research, covering:

- Standard benchmarks with standard quality metrics and standard thresholds. When people are free to define their own quality metrics, there's no way to compare two papers---and no way to trust that an approximation is actually useful. The *de facto* 10% loss standard is my least favorite legacy of [the EnerJ paper][enerj].
- A map of the space of approximation techniques available. If you want to apply approximation to a bottleneck in your favorite application, where should you start? This kind of mapping is common for traditional performance optimization, so we should have one too.
- Agreement on what *kinds* of quality guarantees are worth striving for. Where do statistical guarantees make sense, and where do we need more traditional deterministic bounds?

I'm excited about this idea for a community-sanctioned set of standards, but it's going to be difficult because it doesn't fit with normal academic incentives: this won't lead to a bunch of publications.

### A Better Workshop Next Year

Things I learned about running the conference.
- I love lightning talks, but (a) people put too much effort into them, so we didn't have enough, and (b) reports were that 60 seconds is not enough to get enough interesting stuff across.
- More discussion. Can we get round tables? Questions on cards? The closest we got was the lunchtime discussion, which people liked, especially the "new people."
- Remind people to skip the background/motivation in their talks.

Get more serious about the organization so more people take on the work, so we can scale up the workshop.
Thanks for volunteering if you already did! And let me know if you want to be involved.
